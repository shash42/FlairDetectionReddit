{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/python3\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 1: Reddit Data Collection </h1>\n",
    "I will first be extracting 12 flairs. For each flair, I want to extract 100 reddit posts. For each post, I will be extracting the following data: Title, Body, Comments, URL, Score, Comments Number, Create date.\n",
    "One important choice I had to make was how many comments to extract. I have decided to perform a Breadth First Search, taking only long comments (>200 chars). The rationale is explained during Exploratory Data Analysis. Some posts naturally have far more comments (thousands) than others (less than 10). Thus, I have not extracted all comments. Instead I have employed a heuristic that doesn't replace the 'View more comments' option beyond a certain number of times for posts with more comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flair Number: 1 AskIndia\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "663\n",
      "Flair Number: 2 Non-Political\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "527\n",
      "Flair Number: 3 Scheduled\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "1660\n",
      "Flair Number: 4 Photography\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "63\n",
      "Flair Number: 5 Science/Technology\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "178\n",
      "Flair Number: 6 Politics\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "741\n",
      "Flair Number: 7 Business/Finance\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "172\n",
      "Flair Number: 8 Policy/Economy\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "336\n",
      "Flair Number: 9 Sports\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "84\n",
      "Flair Number: 10 Food\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "158\n",
      "Flair Number: 11 Coronavirus\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "81\n",
      "Flair Number: 12 CAA-NRC-NPR\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "flairs = [\"AskIndia\", \"Non-Political\", \"Scheduled\", \"Photography\", \"Science/Technology\", \"Politics\", \"Business/Finance\", \"Policy/Economy\", \"Sports\", \"Food\", \"Coronavirus\", \"CAA-NRC-NPR\"]\n",
    "reddit = praw.Reddit(client_id='EGV0l4iNPHX3zw', \\\n",
    "                     client_secret='cfZJqAtGpacEHzjzo6ala9IXhOc', \\\n",
    "                     user_agent='flairdetectorscrape', \\\n",
    "                     username='logisbase2', \\\n",
    "                     password='W3codeforlife')\n",
    "subreddit = reddit.subreddit('india')\n",
    "cols = {\"flair\":[], \"title\":[], \"body\":[], \"comments\":[], \"url\":[], \"score\":[], \"comms_num\": [], \"created\": []}\n",
    "flair_num = 0\n",
    "for flair in flairs:\n",
    "    flair_num+=1\n",
    "    print(\"Flair Number: \" + str(flair_num) + \" \" + flair)\n",
    "    \n",
    "    flair_comms=0\n",
    "    get_posts = subreddit.search(\"flair:\"+flair, limit=100)\n",
    "\n",
    "    post_num = 0\n",
    "    for post in get_posts:\n",
    "        post_num+=1\n",
    "        if(post_num%10==0):\n",
    "            print(post_num) \n",
    "        cols[\"flair\"].append(flair)\n",
    "        cols[\"title\"].append(post.title)\n",
    "        cols[\"body\"].append(post.selftext)\n",
    "        cols[\"url\"].append(post.url)\n",
    "        cols[\"score\"].append(post.score)\n",
    "        cols[\"comms_num\"].append(post.num_comments)\n",
    "        cols[\"created\"].append(post.created)\n",
    "        \n",
    "        comm_more = None\n",
    "        if(post.num_comments > 2000):\n",
    "            comm_more = 3\n",
    "        elif(post.num_comments > 500):\n",
    "            comm_more = 7\n",
    "        elif(post.num_comments > 100):\n",
    "            comm_more = 15\n",
    "        char_threshold = 200\n",
    "        count_threshold = 20\n",
    "        count = 0\n",
    "        comment_list = []\n",
    "        post.comments.replace_more(limit=comm_more)\n",
    "        comment_queue = post.comments[:]  # Start with top-level comments\n",
    "        while comment_queue:\n",
    "            comment = comment_queue.pop(0)\n",
    "            if(len(comment.body) > char_threshold):\n",
    "                count+=1\n",
    "                flair_comms+=1\n",
    "            comment_list.append(comment.body)\n",
    "            if(count >= count_threshold):\n",
    "                break\n",
    "        cols[\"comments\"].append(comment_list)\n",
    "    print(flair_comms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)\n",
    "\n",
    "topics_data = pd.DataFrame(cols)\n",
    "_timestamp = topics_data[\"created\"].apply(get_date)\n",
    "topics_data = topics_data.assign(timestamp = _timestamp)\n",
    "del topics_data['created']\n",
    "topics_data.to_csv('reddit-india-data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cell converts the time from UNIX to UTC and loads the data in a csv. <br>\n",
    "Unfortunately CAA-NRC-CAB has only 40-50 posts and thus will not be used as a flair in further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
