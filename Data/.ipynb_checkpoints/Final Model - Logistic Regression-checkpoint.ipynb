{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import ast\n",
    "import pickle\n",
    "import re\n",
    "from nltk.corpus import words\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flairs = ['AskIndia', 'Non-Political', 'Scheduled', 'Photography', 'Science/Technology', 'Politics', 'Business/Finance', 'Policy/Economy', 'Sports', 'Food', 'Coronavirus']\n",
    "label_to_id = {'AskIndia': 0,\n",
    " 'Non-Political': 1,\n",
    " 'Scheduled': 2,\n",
    " 'Photography': 3,\n",
    " 'Science/Technology': 4,\n",
    " 'Politics': 5,\n",
    " 'Business/Finance': 6,\n",
    " 'Policy/Economy': 7,\n",
    " 'Sports': 8,\n",
    " 'Food': 9,\n",
    " 'Coronavirus': 10}\n",
    "\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('reddit-india-data.csv', nrows=1100)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "REPLACE_BY_SPACE_URL = re.compile('[-_]')\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "def clean_url(row):\n",
    "    row = REPLACE_BY_SPACE_URL.sub(' ', row)\n",
    "    initial = (row.split())[0]\n",
    "    row = ' '.join((row.split())[1:])\n",
    "    row = (re.split(\"[,.\\-!?:/]+\", row))\n",
    "    initial = initial.split('/')\n",
    "    initial = initial[len(initial) - 1]\n",
    "    row = initial + \" \" + ' '.join(row)\n",
    "    return row\n",
    "\n",
    "def listtostr(listtext):\n",
    "    comm_list = ast.literal_eval(listtext)\n",
    "    comm_out = ' '.join(comm_list)\n",
    "    return comm_out\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words) # remove stopwords from text\n",
    "    return text\n",
    "\n",
    "wordlist = set(words.words())\n",
    "def only_english(text):\n",
    "    global wordlist\n",
    "    rettext = \" \".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in wordlist or not w.isalpha())\n",
    "    return rettext\n",
    "    \n",
    "data.fillna(\"\",inplace = True)\n",
    "data['title'] = data['title'].apply(clean_text)\n",
    "data['body'] = data['body'].apply(clean_text)\n",
    "data['comments'] = data['comments'].apply(listtostr)    \n",
    "data['comments'] = data['comments'].apply(clean_text)\n",
    "data['url'] = data['url'].apply(clean_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7181818181818181\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          AskIndia       0.71      0.67      0.69        33\n",
      "     Non-Political       0.54      0.52      0.53        25\n",
      "         Scheduled       1.00      1.00      1.00        31\n",
      "       Photography       0.86      0.86      0.86        35\n",
      "Science/Technology       0.61      0.61      0.61        28\n",
      "          Politics       0.71      0.78      0.74        37\n",
      "  Business/Finance       0.68      0.68      0.68        28\n",
      "    Policy/Economy       0.53      0.50      0.52        34\n",
      "            Sports       0.89      0.86      0.88        29\n",
      "              Food       0.74      0.74      0.74        27\n",
      "       Coronavirus       0.56      0.61      0.58        23\n",
      "\n",
      "          accuracy                           0.72       330\n",
      "         macro avg       0.71      0.71      0.71       330\n",
      "      weighted avg       0.72      0.72      0.72       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def printmetrics(y_test, y_pred):\n",
    "    \n",
    "    print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "    print(classification_report(y_test, y_pred,target_names=flairs))\n",
    "\n",
    "def logisticreg(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    weights = {\n",
    "    0 : 1.1,    1 : 1,    3 : 0.95,    4 : 1.05,    5 : 1.05,\n",
    "        6: 1.05, 7 : 1.25,    8 : 0.95, 9: 0.95, 10 : 0.85, }\n",
    "    logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', LogisticRegression(n_jobs=1, C=1, max_iter = 1000, class_weight = weights)),\n",
    "                 ])\n",
    "    logreg.fit(X_train, y_train)\n",
    "    with open('model.pickle', 'wb') as f:\n",
    "        pickle.dump(logreg, f)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    printmetrics(y_test, y_pred)\n",
    "    \n",
    "feat = data['title'] + data['body'] + data['url'] + data['comments']\n",
    "split_ratio = 0.3\n",
    "flairclass = data.flair\n",
    "flairclass = flairclass.map(label_to_id)\n",
    "X_train, X_test, y_train, y_test = train_test_split(feat, flairclass, test_size=split_ratio, random_state = 42)\n",
    "logisticreg(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file mostly has code from Model. This has been made to compile the final model and extract it with Pickle so it can be directly loaded into the web application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
